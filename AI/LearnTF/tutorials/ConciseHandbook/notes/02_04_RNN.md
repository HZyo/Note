# 02_04 循环网络

循环神经网络（Recurrent Neural Network, RNN）是一种适宜于处理序列数据的神经网络，被广泛用于语言模型、文本生成、机器翻译等。关于RNN的原理，可以参考：

- [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
- 台湾大学李宏毅教授的《机器学习》课程的 [Recurrent Neural Network (part 1)](https://www.bilibili.com/video/av10590361/?p=36) [Recurrent Neural Network (part 2)](https://www.bilibili.com/video/av10590361/?p=37) 两部分。
- LSTM原理：[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- RNN序列生成：[[Graves2013\]](https://tf.wiki/zh/models.html#graves2013)

这里，我们使用RNN来进行尼采风格文本的自动生成。

这个任务的本质其实预测一段英文文本的接续字母的概率分布。比如，我们有以下句子:

```
I am a studen
```

这个句子（序列）一共有13个字符（包含空格）。当我们阅读到这个由13个字符组成的序列后，根据我们的经验，我们可以预测出下一个字符很大概率是“t”。我们希望建立这样一个模型，输入num_batch个由编码后字符组成的，长为seq_length的序列，输入张量形状为[num_batch, seq_length]，输出这些序列接续的下一个字符的概率分布，概率分布的维度为字符种类数num_chars，输出张量形状为[num_batch, num_chars]。我们从下一个字符的概率分布中采样作为预测值，然后滚雪球式地生成下两个字符，下三个字符等等，即可完成文本的生成任务。

## 1. 数据

首先，还是实现一个简单的 `DataLoader` 类来读取文本，并以字符为单位进行编码。

```python
class DataLoader():
    def __init__(self):
        path = tf.keras.utils.get_file('nietzsche.txt',
            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
        with open(path, encoding='utf-8') as f:
            self.raw_text = f.read().lower()
        # 转码
        self.chars = sorted(list(set(self.raw_text)))
        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))
        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))
        self.text = [self.char_indices[c] for c in self.raw_text]

    def get_batch(self, seq_length, batch_size):
        seq = []
        next_char = []
        for i in range(batch_size):
            index = np.random.randint(0, len(self.text) - seq_length)
            seq.append(self.text[index:index+seq_length])
            next_char.append(self.text[index+seq_length])
        return np.array(seq), np.array(next_char)       # [num_batch, seq_length], [num_batch]
```

## 2. 模型

接下来进行模型的实现。在 `__init__` 方法中我们实例化一个常用的 `BasicLSTMCell` 单元，以及一个线性变换用的全连接层，我们首先对序列进行**One Hot操作**，即将编码 i 变换为一个 n 维向量，其第i位为1，其余均为0。这里n为字符种类数num_char。变换后的序列张量形状为`[num_batch, seq_length, num_chars]`。接下来，我们将序列从头到尾依序送入RNN单元，即将当前时间t的RNN单元状态 `state` 和 t 时刻的序列 `inputs[:, t, :]` 送入RNN单元，得到当前时间 t 的输出 `output` 和下一个时间 t+1 的RNN单元状态。取RNN单元最后一次的输出，通过全连接层变换到num_chars维，即作为模型的输出。

```python
class RNN(tf.keras.Model):
    def __init__(self, num_chars):
        super().__init__()
        self.num_chars = num_chars
        self.cell = tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell', num_units=256)
        self.dense = tf.keras.layers.Dense(units=self.num_chars)

    def call(self, inputs):
        batch_size, seq_length = tf.shape(inputs)
        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]
        state = self.cell.zero_state(batch_size=batch_size, dtype=tf.float32)
        for t in range(seq_length.numpy()):
            output, state = self.cell(inputs[:, t, :], state)
        output = self.dense(output)
        return output
```

> **inputs** 
>
> `inputs.shape == (batch_size, seq_length, num_chars)` 
>
> 所以 t 时刻的序列 是 `inputs[:,t,:]` 
>
> **全连接层** 
>
> RNN的细胞数 `num_units` 是固定的 `256`，但文本所涉及的单词种类数为 `num_chars` 
>
> 因此需要进行一次转码

## 3. 训练

```python
num_batches = 5000
batch_size = 10
learning_rate = 0.001
seq_length = 10

data_loader = DataLoader()
model = RNN(len(data_loader.chars))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)

for batch_index in range(num_batches):
    X, y = data_loader.get_batch(seq_length, batch_size)
    with tf.GradientTape() as tape:
        y_logit_pred = model(tf.convert_to_tensor(X))
        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)
        print("batch %d: loss %f" % (batch_index, loss.numpy()))
    grads = tape.gradient(loss, model.variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))
```

## 4. 预测

关于文本生成的过程有一点需要特别注意。之前，我们一直使用 `tf.argmax()` 函数，将对应概率最大的值作为预测值。然而对于文本生成而言，这样的预测方式过于绝对，会使得生成的文本失去丰富性。于是，我们使用 `np.random.choice()` 函数按照生成的概率分布取样。这样，即使是对应概率较小的字符，也有机会被取样到。同时，我们加入一个 `temperature` 参数控制分布的形状，参数值越大则分布越平缓（最大值和最小值的差值越小），生成文本的丰富度越高；参数值越小则分布越陡峭，生成文本的丰富度越低。

```python
 # class RNN(tf.keras.Model):
    def predict(self, inputs, temperature=1.):
        batch_size, _ = tf.shape(inputs)
        logits = self(inputs)
        prob = tf.nn.softmax(logits / temperature).numpy()
        return np.array([np.random.choice(self.num_chars, p=prob[i, :])
                         for i in range(batch_size.numpy())])
```

生成文本

```python
X_, _ = data_loader.get_batch(seq_length, 1)
for diversity in [0.2, 0.5, 1.0, 1.2]:
    X = X_
    print("diversity %f:" % diversity)
    for t in range(400):
        y_pred = model.predict(X, diversity)
        print(data_loader.indices_char[y_pred[0]], end='', flush=True)
        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)
```

