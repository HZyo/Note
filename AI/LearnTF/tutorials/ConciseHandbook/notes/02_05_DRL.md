# 02_05 深度强化学习

[强化学习](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0) （Reinforcement learning，RL）强调如何基于环境而行动，以取得最大化的预期利益。结合了深度学习技术后的强化学习更是如虎添翼。这两年广为人知的AlphaGo即是深度强化学习的典型应用。深度强化学习的基础知识可参考：

- [Demystifying Deep Reinforcement Learning](https://ai.intel.com/demystifying-deep-reinforcement-learning/) （[中文翻译](https://snowkylin.github.io/rl/2017/01/04/Reinforcement-Learning.html)）
- [[Mnih2013\]](https://tf.wiki/zh/models.html#mnih2013) 

## 0. 原理

### 0.1 问题

在一个**环境状态**下，**机器人**要采取**动作**使得**未来收益**最大，之后环境**改变状态**，并**反馈收益**。

机器人通过跟环境**交互**，最终**预计未来收益**与**真实未来收益**相近。

### 0.2 奖励

$$
\begin{align}
R_t&=r_t+r_{t+1}+r_{t+2}+...\\
R_t&=r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}\\
   &=r_t+\gamma(r_{t+1}+\gamma r_{t+2}+...)\\
   &=r_t+\gamma R_{t+1}
\end{align}
$$

其中 $r_t​$ 是时刻 `t` 的真实奖励，$R_t​$ 是**预计**的从时刻 `t` 至结束的奖励，由于不确定性，预计的奖励要衰减 $\gamma​$。

### 0.3 动作奖励

给定 $(s,a,r,s')​$，有
$$
Q(s,a)=r+\max_{a'} \gamma Q(s',a')
$$
$Q: (\mathbb{S},\mathbb{A})\to\mathbb{R}$ 

### 0.4 想法

状态过多（如图像，`width * height * channel`），表形式的 $Q(s,a)​$ 难以学习。

利用CNN来计算 $Q(s,a)$ 可以解决这个问题，这里转变一下问题，让 CNN 直接计算
$$
Q:\mathbb{S}\to \{\mathbb{R_a}|a\in\mathbb{A}\}
$$
**损失函数**为
$$
L=\frac{1}{2}(r+\gamma \max_{a'}Q(s',a')-Q(s,a))^2
$$
实现细节包括**回放系统** $\mathbb{D}=\{s,a,r,s'\}$，**探索** $P(\text{random action}|s')=\epsilon$ （从1衰减到0.1）。

## 1. 游戏

这里，我们使用深度强化学习玩CartPole（平衡杆）游戏。简单说，我们需要让模型控制杆的左右运动，以让其一直保持竖直平衡状态。

![000](https://raw.githubusercontent.com/Ubpa/ImgBed/master/Note/AI/LearnTF/tutorials/ConciseHandbook/notes/000.gif)

我们使用 [OpenAI推出的Gym环境库](https://gym.openai.com/) 中的CartPole游戏环境，具体安装步骤和教程可参考 [官方文档](https://gym.openai.com/docs/)和 [这里](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-4-gym/) 。

> ### 安装
>
> ```python
> pip install gym
> ```

```python
import gym

env = gym.make('CartPole-v1')       # 实例化一个游戏环境，参数为游戏名称
state = env.reset()                 # 初始化环境，获得初始状态
while True:
    env.render()                    # 对当前帧进行渲染，绘图到屏幕
    action = model.predict(state)   # 假设我们有一个训练好的模型，能够通过当前状态预测出这时应该进行的动作
    next_state, reward, done, info = env.step(action)   # 让环境执行动作，获得执行完动作的下一个状态，动作的奖励，游戏是否已结束以及额外信息
    if done:                        # 如果游戏结束则退出循环
        break
```

## 2. 模型

那么，我们的任务就是训练出一个模型，能够根据当前的状态预测出应该进行的一个好的动作。粗略地说，一个好的动作应当能够最大化整个游戏过程中获得的奖励之和，这也是强化学习的目标。

这里使用的状态并非是图片，而是4个变量（具体未知，应该就是角度，位置，速度等简单信息）

```python
# Q-network用于拟合Q函数，和前节的多层感知机类似。输入state，输出各个action下的Q-value（CartPole下为2维）。
class QNetwork(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)
        self.dense3 = tf.keras.layers.Dense(units=2)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        return x

    def predict(self, inputs):
        q_values = self(inputs)
        return tf.argmax(q_values, axis=-1)
```

## 3. 训练

```python
num_episodes = 128
num_exploration_episodes = 64
max_len_episode = 1000
batch_size = 32
learning_rate = 1e-3
gamma = 1.
initial_epsilon = 1.
final_epsilon = 0.01

model = QNetwork()
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
replay_buffer = deque(maxlen=10000)

for episode_id in range(num_episodes):
    # 初始化环境，获得初始状态
    state = env.reset()
    epsilon = max(
        initial_epsilon * (num_exploration_episodes - episode_id) / num_exploration_episodes,
        final_epsilon)

    for t in range(max_len_episode):
        env.render()                # 对当前帧进行渲染，绘图到屏幕
        if random.random() < epsilon:               # epsilon-greedy探索策略
            action = env.action_space.sample()      # 以epsilon的概率选择随机动作
        else:
            action = model.predict(
                tf.constant(np.expand_dims(state, axis=0), dtype=tf.float32)).numpy()
            action = action[0]

        # 让环境执行动作，获得执行完动作的下一个状态，动作的奖励，游戏是否已结束以及额外信息
        next_state, reward, done, info = env.step(action)
        # 如果游戏Game Over，给予大的负奖励
        reward = -10. if done else reward
        # 将(state, action, reward, next_state)的四元组（外加done标签表示是否结束）放入经验重放池
        replay_buffer.append((state, action, reward, next_state, 1 if done else 0))
        state = next_state

        # 游戏结束则退出本轮循环，进行下一个episode
        if done:
            print("episode %d, epsilon %f, score %d" % (episode_id, epsilon, t))
            break

        if len(replay_buffer) >= batch_size:
            # 从经验回放池中随机取一个batch的四元组，并分别转换为NumPy数组
            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(
                *random.sample(replay_buffer, batch_size))
            batch_state, batch_reward, batch_next_state, batch_done = \
                [np.array(a, dtype=np.float32) for a in [batch_state, batch_reward, batch_next_state, batch_done]]
            batch_action = np.array(batch_action, dtype=np.int32)

            q_value = model(tf.constant(batch_next_state, dtype=tf.float32))
            
            # 按照论文计算y值
            y = batch_reward + (gamma * tf.reduce_max(q_value, axis=1)) * (1 - batch_done)
            with tf.GradientTape() as tape:
                # 最小化y和Q-value的距离
                loss = tf.losses.mean_squared_error(
                    labels=y,
                    predictions=tf.reduce_sum(model(tf.constant(batch_state)) *
                                              tf.one_hot(batch_action, depth=2), axis=1)
                )

            # 计算梯度并更新参数
            grads = tape.gradient(loss, model.variables)
            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))
```

