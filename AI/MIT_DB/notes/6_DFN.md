# 6. 深度前馈网络

**深度前馈网络**（ deep feedforward network），也叫作 **前馈神经网络**（ feedforward neural network）或者 **多层感知机**（ multilayer perceptron, MLP），是典型的深度学习模型。 

These models are called **feedforward** because information flows through the function being evaluated from **x**, through the intermediate computations used to define **f**, and finally to the output **y**. 

> x->f->y

在模型的输出和模型本身之间没有 **反馈**（ feedback）连接。当前馈神经网络被扩展成包含反馈连接时，它们被称为 **循环神经网络**（ recurrent neural network） 。

前馈神经网络被称作 **网络**（ network）是因为它们通常用许多不同函数复合在一起来表示。该模型与一个**有向无环图**相关联，而图描述了函数是如何复合在一起的。 

例如，我们有三个函数 $f^{(1)},f^{(2)},f^{(3)}$ 连接在一个链上以形成$f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$。这些链式结构是神经网络中最常用的结构。在这种情况下， $f^{(1)}$ 被称为网络的 **第一层**（ first layer）， $f^{(2)}$ 被称为 **第二层**（ second layer），以此类推。链的全长称为模型的 **深度**（ depth）。正是因为这个术语才出现了 ‘‘深度学习’’ 这个名字。 

前馈网络的最后一层被称为 **输出层**（ output layer）。 

在神经网络训练的过程中，我们让 $f(x)$ 去匹配 $f^∗(x)$ 的值。训练数据为我们提供了在不同训练点上取值的、含有噪声的 $f^*(x)$ 的近似实例。每个样本 x 都伴随着一个标签 $y ≈ f^∗(x)$。训练样本直接指明了输出层在每一点 x 上必须做什么；它必须产生一个接近 y 的值。 但是训练数据并没有直接指明其他层应该怎么做。 学习算法必须决定如何使用这些层来产生想要的输出，但是训练数据并没有说每个单独的层应该做什么。相反，学习算法必须决定如何使用这些层来最好地实现 $f^∗$ 的近似。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为 **隐藏层**（ hidden layer）。 

> 这些网络被称为**神经网络**是因为它们或多或少地受到神经科学的启发。 然而，现代的神经网络研究受到更多的是来自许多数学和工程学科的指引，并且神经网络的目标并不是完美地给大脑建模。 我们最好将前馈神经网络想成是为了实现**统计泛化**而设计出的**函数近似机**，它偶尔从我们了解的大脑中提取灵感，但并不是大脑功能的模型。 

一种理解前馈网络的方式是从线性模型开始，并考虑如何克服它的局限性。 线性模型有明显的缺陷，那就是该模型的能力被局限在线性函数里，所以它无法理解任何两个输入变量间的相互作用。 为了扩展线性模型来表示 x 的非线性函数，我们可以不把线性模型用于 x 本身，而是用在一个变换后的输入 ϕ(x) 上，这里 ϕ 是一个非线性变换。 我们可以认为 ϕ 提供了一组描述 x 的**特征**，或者认为它提供了 x 的一个新的**表示**(representation)。 

剩下的问题就是如何选择映射 ϕ 

- 通用的 ϕ
- 手动设计的 ϕ，需要对单独任务进行长时间的努力，且难以迁移
- 学习 ϕ。在这种方法中，我们有一个模型 $y = f(\mathbf{x}; \pmb θ; \mathbf{w}) =ϕ(\mathbf{x}; \pmb θ)^⊤\mathbf{w}$。我们现在有两种参数：用于从一大类函数中学习 ϕ 的参数 $\pmb θ$，以及用于将 ϕ(x) 映射到所需的输出的参数 $\mathbf{w}$。 这是深度前馈网络的一个例子，其中 ϕ 定义了一个隐藏层。这是三种方法中唯一一种放弃了训练问题的凸性的，但是利大于弊。在这种方法中，我们将表示参数化为 $ϕ(\mathbf{x}; \pmb θ)$，并且使用优化算法来寻找 θ，使它能够得到一个好的表示。如果我们想要的话，这种方法也可以通过使它变得高度通用以获得第一种方法的优点——我们只需使用一个非常广泛的函数族 $ϕ(\mathbf{x}; \pmb θ)$。这种方法也可以获得第二种方法的优点。人类专家可以将他们的知识编码进网络来帮助泛化，他们只需要设计那些他们期望能够表现优异的函数族 $ϕ(\mathbf{x}; \pmb θ)$ 即可。这种方法的优点是人类设计者只需要寻找正确的函数族即可，而不需要去寻找精确的函数。 

## 6.1 实例：学习XOR

在这个简单的例子中，我们不会关心统计泛化。我们希望网络在这四个点$\mathbb{X} = \{[0, 0]^⊤; [0, 1]^⊤; [1, 0]^⊤; [1, 1]^⊤\}$ 上表现正确。我们会用全部这四个点来训练我们的网络，唯一的挑战是拟合训练集。

评估整个训练集上表现的 MSE 损失函数为 
$$
J(\pmb\theta)=\frac{1}{4}\sum_{\mathbf{x}\in\mathbb{X}}(f^*(\mathbf{x})-f(\mathbf{x};\pmb\theta))^2
$$

> 在应用领域，对于二进制数据建模时， MSE通常并不是一个合适的损失函数。 

假设我们选择一个线性模型， θ包含 w 和 b，那么我们的模型被定义成 
$$
f(\mathbf{x};\mathbf{w},b)=\mathbf{x}^\top\mathbf{w}+b
$$
解得$\mathbf{w}=0,b=\frac{1}{2}$，线性模型仅仅是在任意一点都输出 0.5。 理由如下：

![1547109452441](assets/1547109452441.jpg)

图上的粗体数字标明了学得的函数必须在每个点输出的值。  当 $x_1 = 0$ 时，模型的输出必须随着 $x_2$ 的增大而增大。当 $x_1$ = 1 时，模型的输出必须随着 $x_2$ 的增大而减小。线性模型必须对x2 使用固定的系数 $w_2$。因此，线性模型不能使用 $x_1$ 的值来改变 $x_2$ 的系数，从而不能解决这个问题。 

解决这个问题的其中一种方法是使用一个模型来学习一个不同的特征空间，在这个空间上线性模型能够表示这个解。 

具体来说，我们这里引入一个非常简单的前馈神经网络，它有一层隐藏层并且隐藏层中包含两个单元。具体如下

![1547109661705](assets/1547109661705.jpg)

> 使用两种不同样式绘制的前馈网络的示例。具体来说，这是我们用来解决 XOR 问题的前馈网络。它有单个隐藏层，包含两个单元。 (左) 在这种样式中，我们将每个单元绘制为图中的一个节点。这种风格是清楚而明确的，但对于比这个例子更大的网络，它可能会消耗太多的空间。 (右)在这种样式中，我们将表示每一层激活的整个向量绘制为图中的一个节点。这种样式更加紧凑。有时，我们对图中的边使用参数名进行注释，这些参数是用来描述两层之间的关系的。这里，我们用矩阵 W 描述从 x 到 h 的映射，用向量 w 描述从 h 到 y 的映射。当标记这种图时，我们通常省略与每个层相关联的截距参数。 

网络现在包含链接在一起的两个函数： $\mathbf{h} = f^{(1)}(\mathbf{x}; W; \mathbf{c})$ 和 $\mathbf{y} = f^{(2)}(\mathbf{h}; \mathbf{w}; b)$，完整的模型是
$$
f(\mathbf{x}; W; \mathbf{c}; \mathbf{w}; b) = f^{(2)}(f^{(1)}(\mathbf{x}))
$$


 如果 $f^{(1)}$ 是线性的，那么前馈网络作为一个整体对于输入仍然是线性的。

> 暂时忽略截距项，假设 $f^{(1)}(x) = W^⊤\mathbf{x}$ 并且 $f^{(2)}(\mathbf{h}) = \mathbf{h}^⊤\mathbf{w}$，那么$f(x) = \mathbf{x}^⊤W\mathbf{w}$。我们可以将这个函数重新表示成 $f(x) = \mathbf{x}^⊤\mathbf{w}′$ 其中 $\mathbf{w}′ = W\mathbf{w}$。 

显然，我们必须用**非线性函数**来描述这些特征。大多数神经网络通过**仿射变换**之后紧跟着一个被称为**激活函数**的固定非线性函数来实现这个目标，其中仿射变换由学得的参数控制。 

我们这里使用这种策略，定义 
$$
\mathbf{h}=g(W^\top \mathbf{x}+\mathbf{c})
$$
激活函数 g 通常选择对每个元素分别起作用的函数，有 $h_i = g(\mathbf{x}^⊤W_{:,i} + c_i)$。

在现代神经网络中，默认的推荐是使用由激活函数 $g(z) = \max\{0, z\}$ 定义的 **整流线性单元**（ rectified
linear unit）或者称为 **ReLU**。

![1547110493575](assets/1547110493575.jpg)

我们现在可以指明我们的整个网络是 
$$
f(\mathbf{x}; W; \mathbf{c}; \mathbf{w}; b)=\mathbf{w}^\top \max\{0,W^\top \mathbf{x}+\mathbf{c}\}+b
$$
现在可以给出 XOR 问题的一个解 
$$
\begin{align}
W&=
\begin
{bmatrix}
1 & 1\\
1 & 1\\
\end
{bmatrix}\\

\mathbf{c}&=
\begin
{bmatrix}
0\\
-1\\
\end
{bmatrix}\\

\mathbf{w}&=
\begin
{bmatrix}
1\\
-2\\
\end
{bmatrix}\\

b&=0\\
\end{align}
$$

> 验算
> $$
> \begin{align}
> X&=
> \begin
> {bmatrix}
> 0 & 0\\
> 0 & 1\\
> 1 & 0\\
> 1 & 1\\
> \end
> {bmatrix}\\
> 
> XW+I_{4\times1}\mathbf{c}^\top&=
> \begin
> {bmatrix}
> 0 & -1\\
> 1 & 0\\
> 1 & 0\\
> 2 & 1\\
> \end
> {bmatrix}\\
> 
> \max\{XW+I_{4\times1}\mathbf{c}^\top\}&=
> \begin
> {bmatrix}
> 0 & 0\\
> 1 & 0\\
> 1 & 0\\
> 2 & 1\\
> \end
> {bmatrix}\\
> 
> \max\{XW+I_{4\times1}\mathbf{c}^\top\}\mathbf{w}+I_{4\times1}b&=
> \begin
> {bmatrix}
> 0\\
> 1\\
> 1\\
> 0\\
> \end
> {bmatrix}\\
> \end{align}
> $$
>

## 6.2 基于梯度的学习

我们到目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸。 凸优化从任何一种初始参数出发都会收敛（理论上如此——在实践中也很鲁棒但可能会遇到数值问题）。用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。 

### 6.2.1 代价函数

#### 6.2.1.1 使用最大似然学习条件分布

大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数表示为 
$$
J(\pmb\theta)=-\mathbb{E}_{\mathbf{x},\mathbf{y}\sim\hat{p}_\text{data}}\log p_\text{model}(\mathbf{y}|\mathbf{x})
$$

> 如果模型用高斯分布，那么等价于线性模型均方误差，其实这种等价性并不要求是高斯分布

贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供一个好的指引。 饱和（变得非常平）的函数破坏了这一目标，因为它们把梯度变得非常小。

#### 6.2.1.2 学习条件统计量

可以将学习看作是选择一个函数而不仅仅是选择一组参数

> $$
> f^*=\arg\min_f\mathbb{E}_{\mathbf{x},\mathbf{y}\sim p_\text{data}}||\mathbf{y}-f(\mathbf{x})||^2\\
> f^*(\mathbf{x})=\mathbb{E}_{\mathbf{y}\sim p_\text{data}(\mathbf{y}|\mathbf{x})}[\mathbf{y}]
> $$
>

可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在没必要估计整个 $p(\mathbf{y} | \mathbf{x})$ 分布时。 

### 6.2.2 输出单元

假设前馈网络提供了一组定义为 $\mathbf{h} = f(\mathbf{x}; \pmb\theta)$ 的隐藏特征。

输出层的作用是随后对这些特征进行一些额外的变换来完成整个网络必须完成的任务。 

#### 6.2.2.1 用于高斯输出分布的线性单元

一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元。 

给定特征 $\mathbf{h}$，线性输出单元层产生一个向量 $\hat{\mathbf{y}} = W^⊤\mathbf{h} + \mathbf{b}$。

 线性输出层经常被用来产生条件高斯分布的均值： 
$$
p(\mathbf{y}|\mathbf{x})=\mathcal{N}(\mathbf{y};\hat{\mathbf{y}},I)
$$
最大似然框架也使得学习高斯分布的协方差矩阵更加容易，或更容易地使高斯分布的协方差矩阵作为输入的函数。然而，对于所有输入，协方差矩阵都必须被限定成一个正定矩阵。线性输出层很难满足这种限定，所以通常使用其他的输出单元来对协方差参数化。 

因为线性模型不会饱和，所以它们易于采用基于梯度的优化算法，甚至可以使用其他多种优化算法。

####  6.2.2.2 用于 Bernoulli 输出分布的 sigmoid 单元

神经网络只需要预测 $P (y = 1 | \mathbf{x})$ 即可。为了使这个数是有效的概率，它必须处在区间 [0, 1] 中。 

为满足该约束条件需要一些细致的设计工作。假设我们打算使用线性单元，并且通过阈值来限制它成为一个有效的概率： 
$$
P(y=1|\mathbf{x})=\text{clamp}(\mathbf{w}^\top\mathbf{h}+b,0,1)
$$
这的确定义了一个有效的条件概率分布，但我们无法使用梯度下降来高效地训练它。当 $\mathbf{w}^⊤\mathbf{h} + b$ 处于单位区间外时，模型的输出对其参数的梯度都将为 0。梯度为 0 通常是有问题的，因为学习算法对于如何改善相应的参数不再具有指导意义 

最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用 sigmoid 输出单元结合最大似然来实现的。
sigmoid 输出单元定义为 
$$
\hat{y}=\sigma(\mathbf{w}^\top\mathbf{h}+b)
$$
我们可以认为 sigmoid 输出单元具有两个部分。首先，它使用一个线性层来计算 $z = \mathbf{w}^⊤\mathbf{h} + b$。接着，它使用 sigmoid 激活函数将 z 转化成概率。 
$$
\begin{align}
P(y=1)&=\sigma(z)\\
P(y=0)&=\sigma(-z)\\
P(y)&=\sigma((2y-1)z)\\
\end{align}
$$
我们使用最大似然来学习一个由 sigmoid 参数化的 Bernoulli 分布，它的损失函数为 
$$
\begin{align}
J(\pmb\theta)
&=-\log P(y|\mathbf{x})\\
&=-\log\sigma((2y-1)z)\\
&=\zeta((1-2y)z)
\end{align}
$$

> 通过将损失函数写成 softplus 函数的形式，我们可以看到它仅仅在 (1 - 2y)z 取绝对值非常大的负值时才会饱和。因此饱和只会出现在模型已经得到正确答案时——当 y = 1 且 z 取非常大的正值时，或者y = 0 且 z 取非常小的负值时。当 z 的符号错误时， softplus 函数的变量 (1 - 2y)z可以简化为 |z|。当 |z| 变得很大并且 z 的符号错误时， softplus 函数渐近地趋向于它的变量 |z|。对 z 求导则渐近地趋向于 sign(z)，所以，对于极限情况下极度不正确的z， softplus 函数完全不会收缩梯度。这个性质很有用，因为它意味着基于梯度的学习可以很快地改正错误的 z。

#### 6.2.2.3 用于 Multinoulli 输出分布的 softmax 单元 

**概念** 

softmax 函数最常用作分类器的输出，来表示 n 个不同类上的概率分布。比较少见的是， softmax 函数可以在模型内部使用。

为了推广到具有 n 个值的离散型变量的情况，我们现在需要创造一个向量 $\hat{\mathbf{y}}$，它的每个元素是 $\hat{y}_i = P (y = i | \mathbf{x})$。我们不仅要求每个 $\hat{y}_i$ 元素介于 0 和 1 之间，还要使得整个向量的和为 1，使得它表示一个有效的概率分布。 

首先，线性层预测了未归一化的对数概率： 
$$
\mathbf{z}=W^\top \mathbf{h}+\mathbf{b}
$$
其中$z_i=\log \hat{P}(y=i|\mathbf{x})$。softmax函数可以对z指数化和归一化来获得需要的$\hat{\mathbf{y}}$。softmax的形式为
$$
\text{softmax}(\mathbf{z})_i=\frac{\exp(z_i)}{\sum_j\exp(z_j)}
$$


**饱和性** 

和 logistic sigmoid一样，当使用最大化对数似然训练 softmax 来输出目标值 y时，使用指数函数工作地非常好。 
$$
\log\text{softmax}(\mathbf{z})_i=z_i-\log\sum_j\exp(z_j)
$$

> 上式第一项表示输入 $z_i$ 总是对代价函数有直接的贡献。因为这一项不会饱和，所以即使 $z_i$ 对上式第二项的贡献很小，学习依然可以进行。当最大化对数似然时，第一项鼓励 $z_i$ 被推高，而第二项则鼓励所有的 z 被压低。为了对第二项$\log∑_j \exp(z_j)$ 有一个直观的理解，注意到这一项可以大致近似为 $\max_j z_j$。这种近似是基于对任何明显小于 $\max_j z_j$ 的 $z_k$， $\exp(z_k)$ 都是不重要的。我们能从这种近似中得到的直觉是，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测。如果正确答案已经具有了 softmax 的最大输入，那么 $-z_i$ 项和 $\log∑_j \exp(z_j) ≈ \max_j z_j = z_i$ 项将大致抵消。这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正确分类的样本产生。 



**结果** 

总体来说，未正则化的最大似然会驱动模型去学习一些参数，而这些参数会驱动 softmax 函数来预测在训练集中观察到的每个结果的比率： 
$$
\text{softmax}(\mathbf{z}(\mathbf{x};\pmb\theta))_i\approx
\frac{\sum_{j=1}^m\mathbf{1}_{y^{(j)}=i,\mathbf{x}^{(j)}=\mathbf{x}}}
{\sum_{j=1}^m\mathbf{1}_{\mathbf{x}^{(j)}=\mathbf{x}}}
$$
因为最大似然是一致的估计量，所以只要模型族能够表示训练的分布，这就能保证发生。在实践中，有限的模型能力和不完美的优化将意味着模型只能近似这些比率。



**数值稳定** 
$$
\text{softmax}(\mathbf{z})=\text{softmax}(\mathbf{z}-\max_iz_i)
$$
变换后的形式允许我们在对 softmax 函数求值时只有很小的数值误差，即使是当 z包含极正或者极负的数时。

观察 softmax 数值稳定的变体，可以看到 softmax 函数由它的变量偏离 $\max_i z_i$ 的量来驱动。

 

**z的产生方式** 

softmax 函数的变量 z 可以通过两种方式产生。

最常见的是简单地使神经网络较早的层输出 z 的每个元素，就像先前描述的使用线性层 $\mathbf{z} = W^⊤\mathbf{h} + \mathbf{b}$。虽然很直观，但这种方法是对分布的过度参数化。 

n 个输出总和必须为 1 的约束意味着只有n - 1 个参数是必要的；第 n 个概率值可以通过 1 减去前面 n - 1 个概率来获得。因此，我们可以强制要求 z 的一个元素是固定的。例如，我们可以要求 $z_n = 0$。事实上，这正是 sigmoid 单元所做的。定义 $P (y = 1 | \mathbf{x}) = σ(z)$ 等价于用二维的 z 以及$z_1 = 0$ 来定义 $P (y = 1 | \mathbf{x}) = \text{softmax}(\mathbf{z})_1$。

无论是 n - 1 个变量还是 n 个变量的方法，都描述了相同的概率分布，但会产生不同的学习机制。

在实践中，无论是过度参数化的版本还是限制的版本都很少有差别，并且实现过度参数化的版本更为简单。 



**神经科学角度** 

从神经科学的角度看，有趣的是认为 softmax 是一种在参与其中的单元之间形成竞争的方式： softmax 输出总是和为 1，所以一个单元的值增加必然对应着其他单元值的减少。这与被认为存在于皮质中相邻神经元间的侧抑制类似。在极端情况下（当最大的 ai 和其他的在幅度上差异很大时），它变成了 **赢者通吃**（ winner-take-all）的形式（其中一个输出接近 1，其他的接近 0）。 



**softmax名字的歧义** 

“softmax’’ 的名称可能会让人产生困惑。这个函数更接近于 argmax 函数而不是max 函数。 “soft’’ 这个术语来源于 softmax 函数是连续可微的。 “argmax’’ 函数的结果表示为一个one-hot向量（只有一个元素为 1，其余元素都为 0 的向量），不是连续和可微的。 softmax 函数因此提供了 argmax 的 ‘‘软化’’ 版本。 max 函数相应的软化版本是 $\text{softmax}(\mathbf{z})^⊤\mathbf{z}$。可能最好是把 softmax 函数称为 “softargmax’’，但当前名称已经是一个根深蒂固的习惯了。 

#### 6.2.2.4 其他的输出类型

**神经网络的学习对象** 

一般来说，我们可以认为神经网络表示函数 $f(\mathbf{x}; \pmb\theta)$。这个函数的输出不是对 y值的直接预测。相反， $f(\mathbf{x}; \pmb\theta) = \pmb\omega$ 提供了 y 分布的参数。我们的损失函数就可以表示成 $- \log p(\mathbf{y}; \pmb\omega(\mathbf{x}))$。

> 这句话非常关键，神经网络 学的是 决定分布的参数
>
> 在预测y值的任务中，其实学的是y的高斯分布的均值

**示例** 

例如，我们想要学习在给定 x 时， y 的条件高斯分布的方差。

简单情况下，方差 $σ^2$ 是一个常数，此时有一个解析表达式，这是因为方差的最大似然估计量仅仅是观测值 y 与它们的期望值的差值的平方平均。

一种计算上代价更加高但是不需要写特殊情况代码的方法是简单地将方差作为分布 $p(\mathbf{y} | \mathbf{x})$ 的其中一个属性，这个分布由 $\pmb\omega = f(\mathbf{x}; \pmb\theta)$ 控制。 负对数似然 $- \log p(\mathbf{y}; \pmb\omega(\mathbf{x}))$ 将为代价函数提供一个必要的合适项来使我们的优化过程可以逐渐地学到方差。 

在标准差不依赖于输入的简单情况下，我们可以在网络中创建一个直接复制到 $\pmb\omega$ 中的新参数。这个新参数可以是 σ 本身，或者可以是表示 $σ^2$ 的参数 v，或者可以是表示 $\frac{1}{σ^2}$ 的参数 β，取决于我们怎样对分布参数化。 

我们可能希望模型对不同的 x 值预测出 y 不同的方差。这被称为 异方差（ heteroscedastic）模型。在异方差情况下，我们简单地把方差指定为 $f(\mathbf{x}; \pmb\theta)$ 其中一个输出值。 实现它的典型方法是使用精度而不是方差来表示高斯分布。在多维变量的情况下，最常见的是使用一个对角精度矩阵 $\text{diag}(\pmb\beta)$。

我们必须确保高斯分布的协方差矩阵是正定的。因为精度矩阵的特征值是协方差矩阵特征值的倒数，所以这等价于确保精度矩阵是正定的。 如果我们使用对角矩阵，或者是一个常数乘以单位矩阵1，那么我们需要对模型输出强加的唯一条件是它的元素都为正。如果我们假设 $\mathbf{a}$ 是用于确定对角精度的模型的原始激活，那么可以用 softplus 函数来获得正的精度向量： $β = ζ(\mathbf{a})$。这种相同的策略对于方差或标准差同样适用，也适用于常数乘以单位阵的情况。 

> 学习一个比对角矩阵具有更丰富结构的协方差或者精度矩阵是很少见的。 

**混合密度网络** 

我们经常想要执行多峰回归 (multimodal regression)，即预测条件分布 $p(\mathbf{y} | \mathbf{x})$的实值，该条件分布对于相同的 $\mathbf{x}$ 值在 $\mathbf{y}$ 空间中有多个不同的峰值。在这种情况下，高斯混合是输出的自然表示 (Jacobs et al., 1991; Bishop, 1994)。将高斯混合作为其输出的神经网络通常被称为 **混合密度网络**（ mixture density network）。具有 n 个分量的高斯混合输出由下面的条件分布定义： 
$$
p(\mathbf{y}|\mathbf{x})=\sum_{i=1}^n p(c=i|\mathbf{x})\mathcal{N}(\mathbf{y};\pmb\mu^{(i)}(\mathbf{x}),\pmb\Sigma^{(i)}(\mathbf{x}))
$$
神经网络必须有三个输出

- 定义$p(c=i|\mathbf{x})$ 的向量

  > 混合组件 $p(c = i | \mathbf{x})$：它们由潜变量 c 关联着，在 n 个不同组件上形成 Multinoulli 分布。这个分布通常可以由 n 维向量的 softmax 来获得，以确保这些输出是正的并且和为 1 
  >
  > 我们之所以认为 c 是潜在的，是因为我们不能直接在数据中观测到它：给定输入 x 和目标 y，不可能确切地知道是哪个高斯组件产生 y，但我们可以想象 y 是通过选择其中一个来产生的，并且将那个未被观测到的选择作为随机变量。 

- 对所有的 i 给出  $\pmb\mu^{(i)}(\mathbf{x})$ 的矩阵

- 对所有的 i 给出 $\pmb\Sigma^{(i)}(\mathbf{x})$ 的张量

有报告说基于梯度的优化方法对于混合条件高斯（作为神经网络的输出）可能是不可靠的，部分是因为涉及到除法（除以方差）可能是数值不稳定的（当某个方差对于特定的实例变得非常小时，会导致非常大的梯度）。一种解决方法是 **梯度截断**（ clipgradient）（见第 10.11.1 节），另外一种是**启发式缩放梯度** (Murray and Larochelle,2014)。 

## 6.3 隐藏单元

隐藏单元的设计是一个非常活跃的研究领域，并且还没有许多明确的指导性理论原则。 

整流线性单元是隐藏单元极好的默认选择。 

在实践中，我们可以放心地忽略下面描述的隐藏单元激活函数的不可微性。 

除非另有说明，大多数的隐藏单元都可以描述为接受输入向量 x，计算仿射变换 $\mathbf{z} = W^⊤\mathbf{x} + \mathbf{b}$，然后使用一个逐元素的非线性函数 $g(z)$。大多数隐藏单元的区别仅仅在于激活函数 $g(z)$ 的形式。 

### 6.3.1 整流线性单元及其扩展

整流线性单元使用激活函数 $g(z) = \max\{0, z\}$。

整流线性单元易于优化，因为它们和线性单元非常类似。 

整流线性单元通常作用于仿射变换之上： 
$$
\mathbf{h}=g(W^\top \mathbf{x}+\mathbf{b})
$$
当初始化仿射变换的参数时，可以将 b 的所有元素设置成一个小的正值，例如 0.1。这使得整流线性单元很可能初始时就对训练集中的大多数输入呈现激活状态，并且允许导数通过。

整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。整流线性单元的各种扩展保证了它们能在各个位置都接收到梯度。 

整流线性单元的三个扩展基于当 $z_i < 0$ 时使用一个非零的斜率 $α_i： h_i =g(\mathbf{z}; \pmb\alpha)_i = \max(0, z_i) + α_i \min(0, z_i)$。 

- **绝对值整流**（ absolute value rectification）固定 $α_i = -1$ 来得到 $g(z) = |z|$。它用于图像中的对象识别 (Jarrett et al., 2009a)，其中寻找在输入照明极性反转下不变的特征是有意义的。整流线性单元的其他扩展比这应用地更广泛。

- **渗漏整流线性单元**（ Leaky ReLU） (Maas et al., 2013) 将 $α_i$ 固定成一个类似 0.01 的小值

- **参数化整流线性单元**（ parametric ReLU）或者 **PReLU** 将 $α_i$ 作为学习的参数 (He et al., 2015)。 

maxout 单元（ maxout unit） (Goodfellow et al., 2013a) 进一步扩展了整流线性单元。 maxout 单元将 $\mathbf{z}$ 划分为每组具有 k 个值的组，而不是使用作用于每个元素的函数 g(z)。每个maxout 单元则输出每组中的最大元素： 
$$
g(\mathbf{z})_i=\max_{j\in\mathbb{G}^{(i)}}z_j
$$
这里 $\mathbb{G}^{(i)}$ 是组 i 的输入索引集 $\{(i - 1)k + 1,..., ik\}$。 

maxout 单元可以学习具有多达 k 段的分段线性的凸函数。  

> maxout 单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的 k， maxout 单元可以以任意的精确度来近似任何凸函数。特别地，具有两块的 maxout 层可以学习实现和传统层相同的输入 x 的函数，这些传统层可以使用整流线性激活函数、绝对值整流、渗漏整流线性单元 或参数化整流线性单元，或者可以学习实现与这些都不同的函数。 

每个 maxout 单元现在由 k 个权重向量来参数化，而不仅仅是一个，所以 maxout单元通常比整流线性单元需要更多的正则化。如果训练集很大并且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作得不错。

maxout 单元还有一些其他的优点。在某些情况下，要求更少的参数可以获得一些统计和计算上的优点。具体来说，如果由 n 个不同的线性过滤器描述的特征可以在不损失信息的情况下，用每一组 k 个特征的最大值来概括的话，那么下一层可以获得 k 倍更少的权重数。

> 因为每个单元由多个过滤器驱动， maxout 单元具有一些冗余来帮助它们抵抗一种被称为 **灾难遗忘**（ catastrophic forgetting）的现象，这个现象是说神经网络忘记了如何执行它们过去训练的任务 (Goodfellow et al., 2014a)。 

### 6.3.2 logistic sigmoid与双曲正切函数

在引入整流线性单元之前，大多数神经网络使用 logistic sigmoid 激活函数 
$$
g(z)=\sigma(z)
$$
或者是双曲正切激活函数 
$$
g(z)=\tanh(z)
$$
这些激活函数紧密相关，因为 $\tanh(z) = 2σ(2z) - 1$。 

我们已经看过 sigmoid 单元作为输出单元用来预测二值型变量取值为 1 的概率。与分段线性单元不同， sigmoid 单元在其大部分定义域内都饱和——当 z 取绝对值很大的正值时，它们饱和到一个高值，当 z 取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当 z 接近 0 时它们才对输入强烈敏感。 sigmoid 单元的广泛饱和性会使得基于梯度的学习变得非常困难。因为这个原因，现在不鼓励将它们用作前馈网络中的隐藏单元。当使用一个合适的代价函数来抵消 sigmoid 的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。 

当必须要使用 sigmoid 激活函数时，双曲正切激活函数通常要比 logistic sigmoid 函数表现更好。在 $\tanh(0) = 0$ 而 $σ(0) = \frac{1}{2}$ 的意义上， 它更像是单位函数。因为 tanh 在 0 附近与恒等函数类似，训练深层神经网络 $\hat{y} = \mathbf{w}^⊤\tanh(U^⊤\tanh(V^⊤\mathbf{x}))$ 类似于训练一个线性模型 $\hat{y} = \mathbf{w}^⊤U^⊤V^⊤\mathbf{x}$ ，只要网络的激活能够被保持地很小。这使得训练 tanh 网络更加容易。

> sigmoid 激活函数在除了前馈网络以外的情景中更为常见。循环网络、许多概率模型以及一些自编码器有一些额外的要求使得它们不能使用分段线性激活函数，并且使得 sigmoid 单元更具有吸引力，尽管它存在饱和性的问题。 

### 6.3.3 其他隐藏单元

一般来说，很多种类的可微函数都表现得很好。许多未发布的激活函数与流行的激活函数表现得一样好。 

在新技术的研究和开发期间，通常会测试许多不同的激活函数，并且会发现许多标准方法的变体表现非常好。这意味着，通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。新的隐藏单元类型如果与已有的隐藏单元表现大致相当的话，那么它们是非常常见的，不会引起别人的兴趣。 

其中一种是完全没有激活函数 g(z)。也可以认为这是使用恒等函数作为激活函数的情况。 

如果神经网络的每一层都仅由线性变换组成，那么网络作为一个整体也将是线性的。然而，神经网络的一些层是纯线性也是可以接受的。 

> 考虑具有 n 个输入和 p个输出的神经网络层 $\mathbf{h} = g(W^⊤\mathbf{x} + \mathbf{b})$。我们可以用两层来代替它，一层使用权重矩阵 U，另一层使用权重矩阵 V。如果第一层没有激活函数，那么我们对基于 W 的原始层的权重矩阵进行因式分解。分解方法是计算 $\mathbf{h} = g(V^⊤U^⊤\mathbf{x} + \mathbf{b})$。如果 U 产生了 q 个输出，那么 U 和 V 一起仅包含 (n + p)q 个参数，而 W 包含 np 个参数。如果 q 很小，这可以在很大程度上节省参数。这是以将线性变换约束为低秩的代价来实现的，但这些低秩关系往往是足够的。线性隐藏单元因此提供了一种减少网络中参数数量的有效方法。 

softmax 单元是另外一种经常用作输出的单元（如第 6.2.2.3 节中所描述的），但有时也可以用作隐藏单元。softmax 单元很自然地表示具有 k 个可能值的离散型随机变量的概率分布，所以它们可以用作一种开关。 

其他一些常见的隐藏单元类型包括： 

- **径向基函数**（ radial basis function, RBF）： $h_i = \exp (-\frac{1}{σ_i^2}||W_{:,i} - \mathbf{x}||^2)$。 这个函数在 $\mathbf{x}$ 接近模板 $W_{:,i}$ 时更加活跃。因为它对大部分 $\mathbf{x}$ 都饱和到 0，因此很难优化。 
- **softplus函数**： $g(a) = ζ(a) = \log(1 + e^a)$。这是整流线性单元的平滑版本，由 Dugas et al. (2001) 引入用于函数近似，由 Nair and Hinton (2010a) 引入用于无向概率模型的条件分布。 Glorot et al. (2011a) 比较了 softplus 和整流线性单元，发现后者的结果更好。通常不鼓励使用 softplus 函数。 softplus 表明隐藏单元类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。 
- **硬双曲正切函数**（ hard tanh）：它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的， $g(a) = \max(-1, \min(1, a))=\text{clamp}(a,-1,1)$。它由 Collobert (2004)引入 

## 6.4 架构设计

大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置成链式结构，其中每一层都是前一层的函数。在这种结构中，第一层由下式给出： 
$$
\mathbf{h}^{(1)}=g^{(1)}(W^{(1)\top} \mathbf{x}+\mathbf{b}^{(1)})
$$
第二层由下式给出
$$
\mathbf{h}^{(2)}=g^{(2)}(W^{(2)\top} \mathbf{h}^{(1)} + \mathbf{b}^{(2)})
$$
在这些链式架构中，主要的架构考虑是选择网络的深度和每一层的宽度。我们将会看到，即使只有一个隐藏层的网络也足够适应训练集。更深层的网络通常能够对每一层使用更少的单元数和更少的参数，并且经常容易泛化到测试集，但是通常也更难以优化。对于一个具体的任务，理想的网络架构必须通过实验，观测在验证集上的误差来找到。 

### 6.4.1 万能近似性质和深度

具体来说， **万能近似定理**（ universal approximation theorem） (Hornik et al., 1989;Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种 ‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数。 

> 定义在 Rn 的有界闭集上的任意连续函数是 Borel 可测的 
>
> 万能近似定理已经被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的整流线性单元 (Leshno et al., 1993)。 

前馈网络提供了表示函数的万能系统，在这种意义上，给定一个函数，存在一个前馈网络能够近似该函数。不存在万能的过程既能够验证训练集上的特殊样本，又能够选择一个函数来扩展到训练集上没有的点。

总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。 

### 6.4.2 其他架构上的考虑

许多神经网络架构已经被开发用于特定的任务。 用于计算机视觉的卷积神经网络。用于序列处理的循环神经网络。 

一般的，层不需要连接在链中，尽管这是最常见的做法。许多架构构建了一个主链，但随后又添加了额外的架构特性。

架构设计考虑的另外一个关键点是如何将层与层之间连接起来。 

## 6.5 反向传播和其他的微分算法

当我们使用前馈神经网络接收输入 $\mathbf{x}​$ 并产生输出 $\hat{\mathbf{y}}​$ 时，信息通过网络向前流动。输入 $\mathbf{x}​$ 提供初始信息，然后传播到每一层的隐藏单元，最终产生输出 $\hat{\mathbf{y}}​$。这称之为 **前向传播**（ forward propagation）。在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数 $J(\pmb θ)​$。

**反向传播**（ back propagation）算法 (Rumelhart et al., 1986c)，经常简称为**backprop**，允许来自代价函数的信息通过网络向后流动，以便计算梯度。 

> 计算梯度的解析表达式是很直观的，但是数值化地求解这样的表达式在计算上的代价可能很大。反向传播算法使用简单和廉价的程序来实现这个目标。 

### 6.5.1 计算图

> computation graph

计算图是有向无环图。

使用图中的每一个节点来表示一个变量。变量可以是标量、向量、矩阵、张量、或者甚至是另一类型的变量。 

操作是指一个或多个变量的简单函数。 

不失一般性，我们定义一个操作仅返回单个输出变量。 

如果变量 y 是变量 x 通过一个操作计算得到的， 那么我们画一条从 x 到 y 的有向边。我们有时用操作的名称来注释输出的节点，当上下文很明确时，有时也会省略这个标注。 

> 示例
>
> ![1547268564375](assets/1547268564375.jpg)
>
> (a) $z=xy​$ 
>
> (b) $\hat{y}=\sigma(\mathbf{x}^\top\mathbf{w}+b)​$ 
>
> (c) $H=\max\{0,XW+\mathbf{b}\}​$ 
>
> (d) $\hat{y}=\mathbf{x}^\top\mathbf{w},\mathbf{u}^{(3)}=\lambda\sum_{i}\omega_i^2$ 

### 6.5.2 微积分中的链式法则

**标量** 
$$
\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}
$$
**向量** 
$$
\nabla_\mathbf{x}z=\Big(\frac{\part \mathbf{y}}{\part \mathbf{x}}\Big)^\top \nabla_{\mathbf{y}}z
$$
其中$\frac{\part \mathbf{ y}}{\part \mathbf{x}}$ 是 Jacobian 矩阵

**张量** 
$$
\nabla_{\mathbf{X}}z=\sum_{j}(\nabla_\mathbf{X}\mathbf{Y}_j)\frac{\part z}{\part \mathbf{Y}_j}
$$

### 6.5.3 递归的使用链式法则来实现反向传播

具体来说，许多子表达式可能在梯度的整个表达式中重复若干次。任何计算梯度的程序都需要选择是存储这些子表达式还是重新计算它们几次。 

> ![1547269436209](assets/1547269436209.jpg)
> $$
> \begin{align}
> &\frac{\part z}{\part w}\\
> =&\frac{\part z}{\part y}\frac{\part y}{\part x}\frac{\part x}{\part w}\\
> =&f'(y)f'(x)f'(w) \tag{1} \\
> =&f'(f(f(w)))f'(f(w))f'(w) \tag{2}\\
> \end{align}
> $$
> 式 (1) 建议我们采用的实现方式是，仅计算 f(w) 的值一次并将它存储在变量 x 中。这是反向传播算法所采用的方法。式 (2) 提出了一种替代方法，其中子表达式 f(w) 出现了不止一次。在替代方法中，每次只在需要时重新计算 f(w)。当存储这些表达式的值所需的存储较少时，式 (1) 的反向传播方法显然是较优的，因为它减少了运行时间。然而，式 (2) 也是链式法则的有效实现，并且当存储受限时它是有用的。 

**前向传播计算** 

首先考虑描述如何计算单个标量 $u^{(n)}​$（例如训练样本上的损失函数）的计算图。

 $n_i$ 个输入节点 $u^{(1)}$ 到 $u^{(n_i)}$。 

我们假设图的节点已经以一种特殊的方式被排序（有向无环图的拓扑排序），使得我们可以一个接一个地计算他们的输出，从 $u^{(n_i+1)}$ 开始，一直上升到 $u^{(n)}​$。 

每个节点 $u^{(i)}$ 与操作 $f^{(i)}$ 相关联，并且通过对以下函数求值来得到 
$$
u^{(i)}=f^{(i)}(\mathbb{A}^{(i)})
$$
其中$\mathbb{A}^{(i)}$ 是$u^{(i)}$ 所有父节点的集合

![1547271192089](assets/1547271192089.jpg)

**反向传播计算** 

了执行反向传播，我们可以构造一个依赖于 G 并添加额外一组节点的计算图。这形成了一个子图 B，它的每个节点都是 G 的节点。 B 中的计算和 G 中的计算顺序完全相反，而且B 中的每个节点计算导数 $\frac{\part u^{(n)}}{\part u^{(i)}}​$ 与前向图中的节点 $u^{(i)}​$ 相关联。 这通过对标量输出 $u^{(n)}​$ 使用链式法则来完成：
$$
\frac{\part u^{(n)}}{\part u^{(j)}}=\sum_{i:j\in Pa(u^{(i)})}\frac{\part u^{(n)}}{\part u^{(i)}}\frac{\part u^{(i)}}{\part u^{(j)}}
$$

> $i:j\in Pa(u^{(i)})$ 指：i 满足条件“j 是 i 的父节点”

执行反向传播所需的计算量与 G 中的边的数量成比例，其中每条边的计算包括计算偏导数（节点关于它的一个父节点的偏导数）以及执行一次乘法和一次加法。 

反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。 

![1547271225801](assets/1547271225801.jpg)

这可以从算法 6.2 中看出，反向传播算法访问了图中的节点 $u^{(j)}$ 到节点 $u^{(i)}$ 的每条边一次，以获得相关的偏导数$\frac{\part u^{(i)}}{\part u^{(j)}}$。反向传播因此避免了重复子表达式的指数爆炸。然而，其他算法可能通过对计算图进行简化来避免更多的子表达式，或者也可能通过重新计算而不是存储这些子表达式来节省内存。 

### 6.5.4 全连接MLP中的反向传播计算

![1547274522033](assets/1547274522033.jpg)

![1547274561166](assets/1547274561166.jpg)

### 6.5.5 符号到符号的导数

代数表达式和计算图都对 **符号**（ symbol）或不具有特定值的变量进行操作。这些代数或者基于图的表达式被称为 **符号表示**（ symbolic representation）。当我们实际使用或者训练神经网络时，我们必须给这些符号赋特定的值。我们用一个特定的 **数值**（ numeric value）来替代网络的符号输入 $\mathbf{x}$。

一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值。我们将这种方法称为 符号到数值的微分。这种方法用在诸如 Torch (Collobert et al., 2011b) 和 Caffe (Jia, 2013) 之类的库中。 

另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节点提供了我们所需导数的符号描述。这是 Theano (Bergstra et al., 2010b; Bastien et al., 2012b) 和 TensorFlow (Abadi et al., 2015) 所采用的方法。 这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述。因为导数只是另外一张计算图，我们可以再次运行反向传播，对导数再进行求导就能得到更高阶的导数。 

> 示例
>
> ![1547275451327](assets/1547275451327.jpg)

### 6.5.6 一般化的反向传播

**细节**  

反向传播算法非常简单。为了计算某个标量 z 关于图中它的一个祖先 $\mathbf{x}​$ 的梯度，我们首先观察到它关于 z 的梯度由 $\frac{dz}{dz}=1​$ 给出。然后，我们可以计算对图中 z的每个父节点的梯度，通过现有的梯度乘以产生 z 的操作的 Jacobian。我们继续乘以 Jacobian，以这种方式向后穿过图，直到我们到达 $\mathbf{x}​$。对于从 z 出发可以经过两个或更多路径向后行进而到达的任意节点，我们简单地对该节点来自不同路径上的梯度进行求和。

更正式地，图 $\mathcal{G}$ 中的每个节点对应着一个变量。为了实现最大的一般化，我们将这个变量描述为一个张量 $\mathbf{V}$。张量通常可以具有任意维度，并且包含标量、向量和矩阵。 

我们假设每个变量 $\mathbf{V}$ 与下列子程序相关联： 

- $\text{get_operation}(\mathbf{V})$：它返回用于计算 $\mathbf{V}$ 的操作，代表了在计算图中流入 $\mathbf{V}$ 的边。
- $\text{get_comsumers}(\mathbf{V},\mathcal{G})$： 它返回一组变量，是计算图 $\mathcal{G}$ 中 $\mathbf{V}$ 的子节点。
- $\text{get_inputs}(\mathbf{V},\mathcal{G})$：它返回一组变量，是计算图 $\mathcal{G}$ 中 V 的父节点。 

每个操作 op 也与 bprop 操作相关联。 该 bprop 操作可以计算 Jacobian 向量积。这是反向传播算法能够实现很大通用性的原因。每个操作负责了解如何通过它参与的图中的边来反向传播。 反向传播算法本身并不需要知道任何微分法则。它只需要使用正确的参数调用每个操作的 bprop 方法即可。 正式地， $\text{op.bprop}(inputs; \mathbf{X}; \mathbf{G})$ 必须返回 
$$
\sum_{i}(\nabla_\mathbf{X}\text{op.f}(\text{input})_i)\mathbf{G}_i
$$
这里， inputs 是提供给操作的一组输入， op.f 是操作实现的数学函数， X 是输入，我们想要计算关于它的梯度， G 是操作对于输出的梯度。 

**计算成本** 

计算梯度至多需要 $O(n^2)$ 的操作，因为计算图是有向无环图，它至多有 $O(n^2)$ 条边。

对于实践中常用图的类型，情况会更好。大多数神经网络的代价函数大致是链式结构的，使得反向传播只有 $O(n)$ 的成本。 

这远远胜过简单的方法，简单方法可能需要在指数级的节点上运算。这种潜在的指数级代价可以通过非递归地扩展和重写递归链式法则来看出： 
$$
\frac{\part u^{(n)}}{\part u^{(j)}}=\sum_{\text{path}(u^{(\pi_1)},u^{(\pi_12)},...,u^{(\pi_t)}),\\\text{from }\pi_1=j\text{ to }\pi_t=n}
\prod_{k=2}^t\frac{\part u^{(\pi_k)}}{\part u^{(\pi_{k-1})}}
$$
由于节点 j 到节点 n 的路径数目可以关于这些路径的长度上指数地增长，所以上述求和符号中的项数（这些路径的数目），可能以前向传播图的深度的指数级增长。 产生如此大的成本是因为对于 $\frac{\part u^{(i)}}{\part u^{(j)}}$，相同的计算会重复进行很多次。为了避免这种重新计算，我们可以将反向传播看作一种表填充算法，利用存储的中间结果 $\frac{\part u^{(n)}}{\part u^{(i)}}​$ 来对表进行填充。图中的每个节点对应着表中的一个位置，这个位置存储对该节点的梯度。通过顺序填充这些表的条目，反向传播算法避免了重复计算许多公共子表达式。这种表填充策略有时被称为 **动态规划**（ dynamic programming）。 

### 6.5.7 实例：用于MLP训练的反向传播

### 6.5.8 复杂化

### 6.5.9 深度学习界以外的微分 

深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来，并且在很大程度上发展了自己关于如何进行微分的文化态度。更一般地， 自动微分（ automaticdifferentiation）领域关心如何以算法方式计算导数。这里描述的反向传播算法只是自动微分的一种方法。它是一种称为 反向模式累加（ reverse mode accumulation）的更广泛类型的技术的特殊情况。其他方法以不同的顺序来计算链式法则的子表达式。一般来说，确定一种计算的顺序使得计算开销最小，是困难的问题。找到计算梯度的最优操作序列是 NP 完全问题 (Naumann, 2008)，在这种意义上，它可能需要将代数表达式简化为它们最廉价的形式。 

反向传播保证梯度计算的计算数目和前向计算的计算数目是同一个量级。然而，可能通过对反向传播算法构建的计算图进行简化来减少这些计算量，并且这是 NP 完全问题。诸如 Theano 和 TensorFlow 的实现使用基于匹配已知简化模式的试探法，以便重复地尝试去简化图。 

### 6.5.10 高阶微分

一些软件框架支持使用高阶导数。在深度学习软件框架中，这至少包括 Theano和 TensorFlow。这些库使用一种数据结构来描述要被微分的原始函数，它们使用相同类型的数据结构来描述这个函数的导数表达式。这意味着符号微分机制可以应用于导数（从而产生高阶导数）。 

我们通常对 Hessian 矩阵的性质比较感兴趣。如果我们有函数 $f : \mathbb{R}^n \to \mathbb{R}$，那么 Hessian矩阵的大小是 n × n。在典型的深度学习应用中， n 将是模型的参数数量，可能很容易达到数十亿。因此，完整的 Hessian 矩阵甚至不能表示。典型的深度学习方法是使用 Krylov 方法（ Krylov method），而不是显式地计算 Hessian 矩阵。  

## 6.6 历史小记

前馈网络可以被视为一种高效的非线性函数近似器，它以使用梯度下降来最小化函数近似误差为基础。从这个角度来看，现代前馈网络是一般函数近似任务的几个世纪进步的结晶。 

现代前馈网络的核心思想自 20 世纪 80 年代以来没有发生重大变化。仍然使用相同的反向传播算法和相同的梯度下降方法。 1986 年至 2015 年神经网络性能的大部分改进可归因于两个因素。首先，较大的数据集减少了统计泛化对神经网络的挑战的程度。第二，神经网络由于更强大的计算机和更好的软件基础设施已经变得更大。然而，少量算法上的变化也显著改善了神经网络的性能。 

- 其中一个算法上的变化是用交叉熵族损失函数替代均方误差损失函数。 

- 另一个显著改善前馈网络性能的算法上的主要变化是使用分段线性隐藏单元来替代 sigmoid 隐藏单元，例如用整流线性单元。 

