# 10. 序列建模：循环和递归网络

循环神经网络（ recurrent neural network）或 RNN (Rumelhart et al., 1986c)是一类用于处理序列数据的神经网络。

参数共享的概念体现在每个时间步中使用的相同卷积核。循环神经网络以不同的方式共享参数。输出的每一项是前一项的函数。输出的每一项对先前的输出应用相同的更新规则而产生。这种循环方式导致参数通过很深的计算图共享。 

## 10.1 展开计算图

$$
\boldsymbol { h } ^ { ( t ) } = f \left( \boldsymbol { h } ^ { ( t - 1 ) } , \boldsymbol { x } ^ { ( t ) } ; \boldsymbol { \theta } \right)
$$

![1547538409925](assets/1547538409925.jpg)

## 10.2 循环神经网络

循环神经网络中一些重要的设计模式包括以下几种： 

- 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络

  > 示例
  >
  > ![1547538707812](assets/1547538707812.jpg)
  >
  > 任何图灵可计算的函数都可以通过这样一个有限维的循环网络计算，在这个意义上图的循环神经网络是万能的。 

- 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络

  > 示例
  >
  > ![1547538772956](assets/1547538772956.jpg)
  >
  > 该图中的 RNN 被训练为将特定输出值放入 o 中，并且 o 是允许传播到未来的唯一信息。此处没有从 h 前向传播的直接连接。 o 通常缺乏过去的重要信息，除非它非常高维且内容丰富。这使得该图中的 RNN 不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化。

- 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络

  > 示例
  >
  > ![1547538820403](assets/1547538820403.jpg)

现在我们研究第一种模式的示例中 RNN 的前向传播公式。 

假设使用双曲正切激活函数做激活函数，假定输出是离散的。从 t = 1 到 t = τ 的每个时间步，我们应用以下更新方程 ：
$$
\begin{array} { l } { \boldsymbol { a } ^ { ( t ) } = \boldsymbol { b } + \boldsymbol { W h } ^ { ( t - 1 ) } + \boldsymbol { U } \boldsymbol { x } ^ { ( t ) } } \\ { \boldsymbol { h } ^ { ( t ) } = \tanh \left( \boldsymbol { a } ^ { ( t ) } \right) } \\ { \boldsymbol { o } ^ { ( t ) } = \boldsymbol { c } + \boldsymbol { V } \boldsymbol { h } ^ { ( t ) } } \\ { \hat { \boldsymbol { y } } ^ { ( t ) } = \operatorname { softmax } \left( \boldsymbol { o } ^ { ( t ) } \right) } \end{array}
$$
与 $\boldsymbol{x}$ 序列配对的 $\boldsymbol{y}​$ 的总损失就是所有时间步的损失之和。 

> 例如，L(t) 为给定的 $\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t)}$ 后 $\boldsymbol{y}^{(t)}$ 的负对数似然，则 
> $$
> \begin{array}
> { l } { L \left( \left\{ \boldsymbol { x } ^ { ( 1 ) } , \ldots , \boldsymbol { x } ^ { ( \tau ) } \right\} , \left\{ \boldsymbol { y } ^ { ( 1 ) } , \ldots , \boldsymbol { y } ^ { ( \tau ) } \right\} \right) } \\ 
> { = \sum _ { t } L ^ { ( t ) } } \\ { = - \sum _ { t } \log p _ { \mathrm { model } } \left( y ^ { ( t ) } | \left\{ \boldsymbol { x } ^ { ( 1 ) } , \ldots , \boldsymbol { x } ^ { ( t ) } \right\} \right) } \\ 
> { = -\sum_t \log \Big(\exp\big(o^{(t)}_{y^{(t)}}\big)/\sum_{j}\exp\big(o^{(t)}_j \big)\Big)} \\ 
> \end{array}
> $$
> 其中$p _ { \text { model } } \left( y ^ { ( t ) } | \left\{ \boldsymbol { x } ^ { ( 1 ) } , \ldots , \boldsymbol { x } ^ { ( t ) } \right\} \right)$ 需要读取模型输出向量 $\hat{\boldsymbol{y}}^{(t)}$ 中对应的 $y^{(t)}​$ 项

梯度计算涉及执行一次前向传播（从左到右的传播），接着是由右到左的反向传播。 

运行时间是 $O(τ)$，并且不能通过并行化来降低，因为前向传播图是固有循序的;每个时间步只能一前一后地计算。前向传播中的各个状态必须保存，直到它们反向传播中被再次使用，因此内存代价也是 $O(τ)$。 应用于展开图且代价为 $O(τ)$ 的反向传播算法称为 **通过时间反向传播**（ back-propagation through time, BPTT）。

### 10.2.1 导师驱动过程和输出循环网络

研究第二种模式。其没有那么强大（因为缺乏隐藏到隐藏的循环连接）。因为这个网络缺少隐藏到隐藏的循环，它要求输出单元捕捉用于预测未来的关于过去的所有信息。因为输出单元明确地训练成匹配训练集的目标，它们不太能捕获关于过去输入历史的必要信息，除非用户知道如何描述系统的全部状  态， 并将它作为训练目标的一部分。消除隐藏到隐藏循环的优点在于，任何基于比较时刻 t 的预测和时刻 t 的训练目标的损失函数中的所有时间步都解耦了。因此训练可以并行化，即在各时刻 t 分别计算梯度。因为训练集提供输出的理想值，所以没有必要先计算前一时刻的输出。 

由输出反馈到模型而产生循环连接的模型可用 **导师驱动过程**（ teacher forcing）进行训练。 

![1547543420203](assets/1547543420203.jpg)

导师驱动过程是一种训练技术， 适用于输出与下一时间步的隐藏状态存在连接的 RNN。 (左) 训练时，我们将训练集中正确的输出 $\boldsymbol{y}^{(t)}$ 反馈到 $\boldsymbol{h}^{(t+1)}$。 (右) 当模型部署后，真正的输出通常是未知的。在这种情况下，我们用模型的输出 $\boldsymbol{o}^{(t)}$ 近似正确的输出 $\boldsymbol{y}^{(t)}$，并反馈回模型。 

如果之后网络在开环 (open-loop) 模式下使用，即网络输出（或输出分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。在这种情况下，训练期间该网络看到的输入与测试时看到的会有很大的不同。

### 10.2.2 计算循环神经网络的梯度

研究第一种模式。
$$
\frac { \partial L } { \partial L ^ { ( t ) } } = 1\\
\left( \nabla _ { o ^ { ( t ) } } L \right) _ { i } = \frac { \partial L } { \partial o _ { i } ^ { ( t ) } } = \frac { \partial L } { \partial L ^ { ( t ) } } \frac { \partial L ^ { ( t ) } } { \partial o _ { i } ^ { ( t ) } } = \hat { y } _ { i } ^ { ( t ) } - \mathbf { 1 } _ { i , y ^ { ( t ) } }\\
$$
我们从序列的末尾开始，反向进行计算。在最后的时间步 $τ$, $\boldsymbol{h}^{(τ )}$ 只有 $\boldsymbol{o}^{(τ)}$ 作为后续节点，因此这个梯度很简单： 
$$
\nabla _ { h ^ { ( \tau ) } } L = \boldsymbol { V } ^ { \top } \nabla _ { \boldsymbol { o } ^ { ( \tau ) } } L
$$
然后，我们可以从时刻 $t = τ - 1$ 到 $t = 1$ 反向迭代，通过时间反向传播梯度，注意$\boldsymbol{h}^{(t)}(t < τ)$ 同时具有 $\boldsymbol{h}^{(t)}$ 和 $\boldsymbol{h}^{(t+1)}$ 两个后续节点。因此，它的梯度由下式计算 
$$
\begin{array} { l } { \nabla _ { h ^ { ( t ) } } L = \left( \frac { \partial h ^ { ( t + 1 ) } } { \partial h ^ { ( t ) } } \right) ^ { \top } \left( \nabla _ { h ^ { ( t + 1 ) } } L \right) + \left( \frac { \partial o ^ { ( t ) } } { \partial h ^ { ( t ) } } \right) ^ { \top } \left( \nabla _ { o ^ { ( t ) } } L \right) } \\ { = W ^ { \top } \left( \nabla _ { h ^ { ( t + 1 ) } L ) } \operatorname { diag } \left( 1 - \left( h ^ { ( t + 1 ) } \right) ^ { 2 } \right) + V ^ { \top } \left( \nabla _ { o ^ { ( t ) } } L \right) \right. } \end{array}
$$
关于剩下参数的梯度可以由下式给出​
$$
\begin{align}
\nabla _ { c } L &= \sum _ { t } \left( \frac { \partial \boldsymbol { o } ^ { ( t ) } } { \partial \boldsymbol { c } } \right) ^ { \top } \nabla _ { \boldsymbol { o } ^ { ( t ) } } L = \sum _ { t } \nabla _ { o ^ { ( t ) } } L\\

\nabla _ { b } L &= \sum _ { t } \left( \frac { \partial h ^ { ( t ) } } { \partial b ^ { ( t ) } } \right) ^ { \top } \nabla _ { h ^ { ( t ) } } L = \sum _ { t } \operatorname { diag } \left( 1 - \left( h ^ { ( t ) } \right) ^ { 2 } \right) \nabla _ { h ^ { ( t ) } } L\\

\nabla _ { \boldsymbol { V } } L &= \sum _ { t } \sum _ { i } \left( \frac { \partial L } { \partial o _ { i } ^ { ( t ) } } \right) \nabla _ { V } o _ { i } ^ { ( t ) } = \sum _ { t } \left( \nabla _ { o ^ { ( t ) } } L \right) \boldsymbol { h } ^ { ( t ) ^ { \top } }\\

\nabla _ { W } L 
& = \sum _ { t } \sum _ { i } \left( \frac { \partial L } { \partial h _ { i } ^ { ( t ) } } \right) \nabla _ { W ^ { ( t ) } } h _ { i } ^ { ( t ) } \\ 
& = \sum _ { t } \operatorname { diag } \left( 1 - \left( h ^ { ( t ) } \right) ^ { 2 } \right) \left( \nabla _ { h ^ { ( t ) } } L \right) h ^ { ( t - 1 ) ^ { \top } } \\

\nabla _ { U } L 
& = \sum _ { t } \sum _ { i } \left( \frac { \partial L } { \partial h _ { i } ^ { ( t ) } } \right) \nabla _ { U ^ { ( t ) } } h _ { i } ^ { ( t ) } \\ 
& = \sum _ { t } \operatorname { diag } \left( 1 - \left( h ^ { ( t ) } \right) ^ { 2 } \right) \left( \nabla _ { h ^ { ( t ) } } L \right) x ^ { ( t ) ^ { \top } } \\ 

\end{align}
$$

### 10.2.3 作为有向图模型的循环网络

> 没看懂，喵喵喵

### 10.2.4 基于上下文的 RNN 序列建模

只使用单个向量 $\boldsymbol{x}$ 作为输入。 将额外输入提供到 RNN 的一些常见方法是：

-  在每个时刻作为一个额外输入 

  > 示例
  >
  > ![1547547704979](assets/1547547704979.jpg)
  >
  > 这类 RNN 适用于很多任务如图注，其中单个图像作为模型的输入，然后产生描述图像的词序列。观察到的输出序列的每个元素 $\boldsymbol{y}^{(t)}​$同时用作输入（对于当前时间步）和训练期间的目标（对于前一时间步）。 

- 作为初始状态 $\boldsymbol{h}^{(0)}​$ 

- 结合两种方式

RNN 可以接收向量序列 $\boldsymbol{x}^{(t)}$ 作为输入，而不是仅接收单个向量 $\boldsymbol{x}$ 作为输入。 第一种模式示例描述的RNN在条件独立的假设下的分布为
$$
\prod _ { t } P \left( \boldsymbol { y } ^ { ( t ) } | \boldsymbol { x } ^ { ( 1 ) } , \ldots , \boldsymbol { x } ^ { ( t ) } \right)
$$
为去掉条件独立的假设，我们可以在时刻 t 的输出到时刻 t + 1 的隐藏单元添加连接 

![1547548167267](assets/1547548167267.jpg)

## 10.3 双向RNN

目前为止我们考虑的所有循环神经网络有一个 ‘‘因果’’ 结构，意味着在时刻 t 的状态只能从过去的序列 $\boldsymbol{x} ^ { ( 1 ) } , \ldots , \boldsymbol{x} ^ { ( t - 1 ) }$ 以及当前的输入 $\boldsymbol{x}^{(t)}$ 捕获信息。 

然而，在许多应用中，我们要输出的 $\boldsymbol{y}^{(t)}$ 的预测可能依赖于整个输入序列。 

> 例如，在语音识别中，由于协同发音，当前声音作为音素的正确解释可能取决于未来几个音素，甚至潜在的可能取决于未来的几个词，因为词与附近的词之间的存在语义依赖：如果当前的词有两种声学上合理的解释，我们可能要在更远的未来（和过去）寻找信息区分它们。这在手写识别和许多其他序列到序列学习的任务中也是如此 

双向循环神经网络（或双向 RNN）为满足这种需要而被发明 (Schuster and Paliwal, 1997)。 顾名思义，双向 RNN 结合时间上从序列起点开始移动的 RNN 和另一个时间上从序列末尾开始移动的 RNN。 

> 示例
>
> ![1547548897283](assets/1547548897283.jpg)

## 10.4 基于编码-解码的序列到序列架构

本节我们讨论如何训练RNN，使其将输入序列映射到不一定等长的输出序列。 

用于映射可变长度序列到另一可变长度序列最简单的RNN架构最初由Cho et al. (2014a) 提出，之后不久由Sutskever et al. (2014) 独立开发，并且第一个使用这种方法获得翻译的最好结果。 如下

![1547550341905](assets/1547550341905.jpg)

此架构的一个明显不足是，编码器 RNN 输出的上下文 C 的维度太小而难以适当地概括一个长序列。 这种现象由Bahdanau et al. (2015) 在机器翻译中观察到。他们提出让 C 成为可变长度的序列，而不是一个固定大小的向量。此外，他们还引入了将序列 C 的元素和输出序列的元素相关联的 **注意力机制**（ attention mechanism）。 

## 10.5 深度循环网络

大多数 RNN 中的计算可以分解成三块参数及其相关的变换： 

1. 从输入到隐藏状态
2. 从前一隐藏状态到下一隐藏状态
3. 从隐藏状态到输出

在这些操作中引入深度会有利的吗？实验证据 (Graves et al., 2013; Pascanu et al., 2014a) 强烈暗示理应如此。 

> 示例
>
> ![1547553702985](assets/1547553702985.jpg)
>
> 循环神经网络可以通过许多方式变得更深 (Pascanu et al., 2014a)。 
>
> (a) 隐藏循环状态可以被分解为具有层次的组。
>
>  (b) 可以向输入到隐藏，隐藏到隐藏以及隐藏到输出的部分引入更深的计算 (如 MLP)。这可以延长链接不同时间步的最短路径。
>
>  (c) 可以引入跳跃连接来缓解路径延长的效应。 

## 10.6 递归神经网络

递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不是 RNN 的链状结构，因此是不同类型的计算图。 

> 示例
>
> ![1547553848377](assets/1547553848377.jpg)
>
> 递归网络将循环网络的链状计算图推广到树状计算图。 可变大小的序列 $\boldsymbol{x} ^ { ( 1 ) } , \boldsymbol{x} ^ { ( 2 ) } , \ldots , \boldsymbol{x} ^ { ( t ) }$ 可以通过固定的参数集合（权重矩阵 $\boldsymbol{U}, \boldsymbol{V}, \boldsymbol{W}$）映射到固定大小的表示（输出 $\boldsymbol{o}$）。该图展示了监督学习的情况，其中提供了一些与整个序列相关的目标 $\boldsymbol{y}$。 

递归网络的一个明显优势是，对于具有相同长度 $τ$ 的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从 $τ$ 减小为 $O(\log τ)$，这可能有助于解决长期依赖。 一个悬而未决的问题是如何以最佳的方式构造树。 

## 10.7 长期依赖的挑战

长期依赖的概念详见 8.2.5 

本章的其余部分将讨论目前已经提出的降低学习长期依赖（在某些情况下，允许一个 RNN 学习横跨数百步的依赖）难度的不同方法，但学习长期依赖的问题仍是深度学习中的一个主要挑战。 

## 10.8 回声状态网络

从 $\boldsymbol{h}^{(t-1)}$ 到 $\boldsymbol{h}^{(t)}$ 的循环权重映射以及从 $\boldsymbol{x}^{(t)}$ 到 $\boldsymbol{h}^{(t)}$ 的输入权重映射是循环网络中最难学习的参数。 

研究者 (Jaeger, 2003; Maass et al., 2002; Jaeger and Haas, 2004; Jaeger, 2007b) 提出避免这种困难的方法是设定循环隐藏单元，使其能很好地捕捉过去输入历史，并且只学习输出权重。  

ESN 和流体状态机都被称为 **储层计算**（ reservoir computing） (Lukoševičius and Jaeger, 2009)，因为隐藏单元形成了可能捕获输入历史不同方面的临时特征池。

储层计算循环网络类似于核机器，这是思考它们的一种方式：它们将任意长度的序列（到时刻 t 的输入历史）映射为一个长度固定的向量（循环状态 $\boldsymbol{h}^{(t)}$），之后可以施加一个线性预测算子（通常是一个线性回归）以解决感兴趣的问题。训练准则就可以很容易地设计为输出权重的凸函数。例如，如果输出是从隐藏单元到输出目标的线性回归，训练准则就是均方误差，由于是凸的就可以用简单的学习算法可靠地解决 (Jaeger, 2003)。 

因此，重要的问题是：我们如何设置输入和循环权重才能让一组丰富的历史可以在循环神经网络的状态中表示？ 

回声状态网络的策略是简单地固定权重使其具有一定的谱半径如 3，其中信息通过时间前向传播，但会由于饱和非线性单元（如 tanh）的稳定作用而不会爆炸。 

## 10.9 渗漏单元和其他多时间尺度的策略

### 10.9.1 时间维度的跳跃连接

增加从遥远过去的变量到目前变量的直接连接是得到粗时间尺度的一种方法。 

### 10.9.2 渗漏单元和一系列不同时间尺度

$$
\mu^{(t)}\gets\alpha\mu^{(t-1)}+(1-\alpha)v^{(t)}
$$

我们可以通过两种基本策略设置渗漏单元使用的时间常数。一种策略是手动将其固定为常数，例如在初始化时从某些分布采样它们的值。另一种策略是使时间常数成为自由变量，并学习出来。 

### 10.9.3 删除连接

主动删除长度为一的连接并用更长的连接替换它们。 

## 10.10 长短期记忆和其他门控RNN

### 10.10.1 LSTM

> long short-term memory

![1547556073603](assets/1547556073603.jpg)

LSTM 循环网络‘‘细胞’’ 的框图。细胞彼此循环连接，代替一般循环网络中普通的隐藏单元。这里使用常规的人工神经元计算输入特征。如果 sigmoid 输入门允许，它的值可以累加到状态。状态单元具有线性自循环，其权重由遗忘门控制。细胞的输出可以被输出门关闭。所有门控单元都具有 sigmoid 非线性，而输入单元可具有任意的压缩非线性。状态单元也可以用作门控单元的额外输入。黑色方块表示单个时间步的延迟。 

LSTM 循环网络除了外部的 RNN 循环外，还具有内部的 “LSTM 细胞’’ 循环（自环），因此 LSTM 不是简单地向输入和循环单元的仿射变换之后施加一个逐元素的非线性。 与普通的循环网络类似，每个单元有相同的输入和输出，但也有更多的参数和控制信息流动的门控单元系统。 最重要的组成部分是状态单元 $s_i^{(t)}$，与前一节讨论的渗漏单元有类似的线性自环。然而，此处自环的权重（或相关联的时间常数）由 **遗忘门**（ forget gate） $f_i^{(t)}$ 控制（时刻 t 和细胞 i），由 sigmoid 单元将权重设置为 0 和 1 之间的值： 
$$
f _ { i } ^ { ( t ) } = \sigma \left( b _ { i } ^ { f } + \sum _ { j } U _ { i , j } ^ { f } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } ^ { f } h _ { j } ^ { ( t - 1 ) } \right)
$$
其中 $\boldsymbol{x}^{(t)}$ 是当前输入向量， $\boldsymbol{h}^t$ 是当前隐藏层向量， $\boldsymbol{h}^t$ 包含所有 LSTM 细胞的输出。$\boldsymbol{b}^f$，$\boldsymbol{U}^f$，$\boldsymbol{W}^f$ 分别是偏置、输入权重和遗忘门的循环权重。 

因此 LSTM 细胞内部状态以如下方式更新：
$$
s _ { i } ^ { ( t ) } = f _ { i } ^ { ( t ) } s _ { i } ^ { ( t - 1 ) } + g _ { i } ^ { ( t ) } \sigma \left( b _ { i } + \sum _ { j } U _ { i , j } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } h _ { j } ^ { ( t - 1 ) } \right)
$$
其中 $\boldsymbol{b}$，$\boldsymbol{U}$，$\boldsymbol{W}$ 分别是 LSTM 细胞中的偏置、输入权重和遗忘门的循环权重。 

外部输入门 (external input gate) 单元 $g_i^{(t)}$ 以类似遗忘门（使用sigmoid获得一个 0 和 1 之间的值）的方式更新，但有自身的参数： 
$$
g _ { i } ^ { ( t ) } = \sigma \left( b _ { i } ^ { g } + \sum _ { j } U _ { i , j } ^ { g } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } ^ { g } h _ { j } ^ { ( t - 1 ) } \right)
$$
LSTM 细胞的输出 $h_i^{(t)}$ 也可以由输出门 (output gate) $q_i^{(t)}$ 关闭（使用sigmoid单元作为门控）： 
$$
\begin{array} { l } { h _ { i } ^ { ( t ) } = \tanh \left( s _ { i } ^ { ( t ) } \right) q _ { i } ^ { ( t ) } } \\ { q _ { i } ^ { ( t ) } = \sigma \left( b _ { i } ^ { o } + \sum _ { j } U _ { i , j } ^ { o } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } ^ { o } h _ { j } ^ { ( t - 1 ) } \right) } \end{array}
$$
其中 $\boldsymbol{b}^o$，$\boldsymbol{U}^o$，$\boldsymbol{W}^o$ 分别是偏置、输入权重和遗忘门的循环权重。 

在这些变体中，可以选择使用细胞状态 $s_i^{(t)}$ 作为额外的输入（及其权重），输入到第 i 个单元的三个门，如图所示。这将需要三个额外的参数。 

### 10.10.2 其他门控RNN

门控循环单元或 GRU，与 LSTM 的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元的决定。

更新公式如下：
$$
h _ { i } ^ { ( t ) } = u _ { i } ^ { ( t - 1 ) } h _ { i } ^ { ( t - 1 ) } + \left( 1 - u _ { i } ^ { ( t - 1 ) } \right) \sigma \left( b _ { i } + \sum _ { j } U _ { i , j } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } r _ { j } ^ { ( t - 1 ) } h _ { j } ^ { ( t - 1 ) } \right)
$$
其中 $\boldsymbol{u}$ 代表 ‘‘更新’’ 门， $\boldsymbol{r}$ 表示 ‘‘复位’’ 门。它们的值就如通常所定义的： 
$$
\begin{array} { l } { u _ { i } ^ { ( t ) } = \sigma \left( b _ { i } ^ { u } + \sum _ { j } U _ { i , j } ^ { u } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } ^ { u } h _ { j } ^ { ( t ) } \right) } \\ { r _ { i } ^ { ( t ) } = \sigma \left( b _ { i } ^ { r } + \sum _ { j } U _ { i , j } ^ { r } x _ { j } ^ { ( t ) } + \sum _ { j } W _ { i , j } ^ { r } h _ { j } ^ { ( t ) } \right) } \end{array}
$$
复位和更新门能独立地 ‘‘忽略’’ 状态向量的一部分。更新门像条件渗漏累积器一样可以线性门控任意维度，从而选择将它复制（在 sigmoid 的一个极端）或完全由新的‘‘目标状态’’ 值（朝向渗漏累积器的收敛方向）替换并完全忽略它（在另一个极端）。复位门控制当前状态中哪些部分用于计算下一个目标状态，在过去状态和未来状态之间引入了附加的非线性效应。 

## 10.11 优化长期依赖

### 10.11.1 截断梯度

一种选择是在参数更新之前， 逐元素地截断小批量产生的参数梯度 (Mikolov, 2012)。另一种是在参数更新之前截断梯度 g 的范数 $∥g∥$ (Pascanu et al., 2013a)： 
$$
\begin{aligned} 
\text { if }  \| \boldsymbol{g} \|& > v \\ 
& \boldsymbol{g} \leftarrow \frac { \boldsymbol{g} v } { \| \boldsymbol{g} \| } 
\end{aligned}
$$
事实上，当梯度大小高于阈值时，即是采取简单的随机步骤往往工作得几乎一样好。通常会离开数值不稳定的状态。 

### 10.11.2 引导信息流的正则化

$$
\Omega = \sum _ { t } \left( \frac { \left\| \left( \nabla _ { h ^ { ( t ) } } L \right) \frac { \partial h ^ { ( t ) } } { \partial h ^ { ( t - 1 ) } } \right\| } { \left\| \nabla _ { h ^ { ( t ) } } L \right\| } - 1 \right) ^ { 2 }
$$

## 10.12 外显记忆

神经网络擅长存储隐性知识，但是他们很难记住事实。被存储在神经网络参数中之前，随机梯度下降需要多次提供相同的输入，即使如此，该输入也不会被特别精确地存储。 Graves et al. (2014) 推测这是因为神经网络缺乏**工作存储** (working memory) 系统，即类似人类为实现一些目标而明确保存和操作相关信息片段的系统。 

为了解决这一难题， Weston et al. (2014) 引入了 **记忆网络**（ memory network），其中包括一组可以通过寻址机制来访问的记忆单元。 

记忆网络原本需要监督信号指示他们如何使用自己的记忆单元。 Graves et al. (2014) 引入的 神经网络图灵机
（ neural Turing machine），不需要明确的监督指示采取哪些行动而能学习从记忆单元读写任意内容，并通过使用基于内容的软注意机制（ 见Bahdanau et al. (2015)和第 12.4.5.1 节），允许端到端的训练。这种软寻址机制已成为其他允许基于梯度优化的模拟算法机制的相关架构的标准 (Sukhbaatar et al., 2015; Joulin and Mikolov, 2015; Kumar et al., 2015a; Vinyals et al., 2015a; Grefenstette et al., 2015)。 

每个记忆单元可以被认为是 LSTM 和 GRU 中记忆单元的扩展。不同的是，网络输出一个内部状态来选择从哪个单元读取或写入，正如数字计算机读取或写入到特定地址的内存访问。 

产生确切整数地址的函数很难优化。为了缓解这一问题， NTM 实际同时从多个记忆单元写入或读取。读取时，它们采取许多单元的加权平均值。写入时，他们对多个单元修改不同的数值。 

这些记忆单元通常扩充为包含向量，而不是由 LSTM 或 GRU 存储单元所存储的单个标量。增加记忆单元大小的原因有两个。原因之一是，我们已经增加了访问记忆单元的成本。我们为产生用于许多单元的系数付出计算成本，但我们预期这些系数聚集在周围小数目的单元。通过读取向量值，而不是一个标量，我们可以抵消部分成本。使用向量值的记忆单元的另一个原因是，它们允许基于内容的寻址(content-based addressing)，其中从一个单元读或写的权重是该单元的函数。如果我们能够生产符合某些但并非所有元素的模式，向量值单元允许我们检索一个完整向量值的记忆。 

![1547558609595](assets/1547558609595.jpg)

