# C13. 蒙特卡洛积分 Monte Carlo Integeration

[TOC]

许多积分方程没有解析解，需要采用数值方法。尽管标准的数值积分方法如梯形积分和高斯求积能高效求解低维平滑积分，他们的收敛速度在高维和不连续积分时会变得糟糕。

蒙特卡罗数值积分法是解决这一问题的一种方法。他们利用随机性来计算积分，其收敛速度与被积函数的维数无关。

合理地使用随机性已经彻底改变了算法设计领域。随机算法可大致地分为两类

- Las Vegas：使用随机性，但最终结果相同。
- Monte Carlo：依赖于随机数，给出不同的结果，但平均上结果正确。

蒙特卡洛的一个非常有用的性质是，只要有能力计算被积函数 $f(x)$ 在域内任意点的值，就能估计积分 $\int f(x)\mathrm{d}x$。

渲染中遇到的许多积分很难或者不可能直接计算。而这些问题可以用蒙特卡洛积分来解决。

蒙特卡洛的主要缺点：如果用 n 个样本来估计积分，算法收敛到正确结果的速率是 $O(n^{-1/2})$。换句话说，如果要减少一半的误差，需要 4 倍的样本。

## 13.1 背景与概率回顾 Background and Probability Review

随机变量 $X$ 是一个从某个随机过程选择的值。一个函数作用在随机变量 $X$ 上得到新的随机变量 $Y=f(X)$。

采样离散随机变量 $X$，其满足 $p_i=P(X=X_i)$，我们可以用一个连续均匀分布的随机变量 $\xi\in[0,1)$，然后将其映射到离散随机变量上，选择 $X_i$ 如果
$$
\sum _ { j = 1 } ^ { i - 1 } p _ { j } < \xi \leq \sum _ { j = 1 } ^ { i } p _ { j }
$$
对于光照任务，概率基于功率，为
$$
p _ { i } = \frac { \Phi _ { i } } { \sum _ { j } \Phi _ { j } }
$$
随机变量的累积分布函数 cumulative distribution function (CDF) $P(x)$ 定义如下
$$
P ( x ) = \operatorname { Pr } \{ X \leq x \}
$$

### 13.1.1 连续随机变量 Continuous Random Variables

一个特别重要的随机变量是标准均匀随机变量 canonical uniform random variable，记为 $\xi$，在 $[0,1)$ 上等概率取值。

重要性体现在两方面

- 容易从软件层面实现这个随机变量
- 可以用该随机变量和适当的变换来生成任意分布的样本

概率分布函数 probability density function PDF 描述随机变量取特定值的相对概率，定义为
$$
p ( x ) = \frac { \mathrm { d } P ( x ) } { \mathrm { d } x }
$$
对于 $\xi$，我们有
$$
p ( x ) = \left\{ \begin{array} { l l } { 1 } & { x \in [ 0,1 ) } \\ { 0 } & { \text { otherwise } } \end{array} \right.
$$
PDFs 要求非负和定义域上积分值为 1。

区间概率
$$
P ( x \in [ a , b ] ) = \int _ { a } ^ { b } p ( x ) \mathrm { d } x
$$

### 13.1.2 期望和方差 Expected Values and Variance

函数 $f$ 的期望值 $E_p[f (x)]$ 定义为函数在其定义域上的某个值的分布 $p(x)$ 上的平均值。域 D 上的期望值定义为
$$
E _ { p } [ f ( x ) ] = \int _ { D } f ( x ) p ( x ) \mathrm { d } x
$$
函数的方差是函数与期望值的平方差的期望，即
$$
V [ f ( x ) ] = E \left[ ( f ( x ) - E [ f ( x ) ] ) ^ { 2 } \right]
$$
期望满足线性性
$$
\begin{aligned} E [ a f ( x ) ] & = a E [ f ( x ) ] \\ E \left[ \sum _ { i } f \left( X _ { i } \right) \right] & = \sum _ { i } E \left[ f \left( X _ { i } \right) \right] \end{aligned}
$$
方差满足
$$
\begin{aligned}
V [ a f ( x ) ] &= a ^ { 2 } V [ f ( x ) ]\\
\sum _ { i } V \left[ f \left( X _ { i } \right) \right] &= V \left[ \sum _ { i } f \left( X _ { i } \right) \right]\\
V [ f ( x ) ] &= E \left[ ( f ( x ) ) ^ { 2 } \right] - E [ f ( x ) ] ^ { 2 }\\
\end{aligned}
$$

## 13.2 蒙特卡洛估计 the Monte Carlo Estimator

假设我们要计算 1D 积分 $\int _ { a } ^ { b } f ( x ) \mathrm { d } x$。给定一组均匀随机变量 $X_i\in[a,b]$，蒙特卡洛估计表明估计式
$$
F _ { N } = \frac { b - a } { N } \sum _ { i = 1 } ^ { N } f \left( X _ { i } \right)
$$
的期望值 $E[F_N]$ 等于积分值。

> **推导** 
>
> 已知随机变量 $X_i$ 的 PDF $p(x)=1/(b-a)$，可推得
> $$
> \begin{aligned} E \left[ F _ { N } \right] & = E \left[ \frac { b - a } { N } \sum _ { i = 1 } ^ { N } f \left( X _ { i } \right) \right] \\ & = \frac { b - a } { N } \sum _ { i = 1 } ^ { N } E \left[ f \left( X _ { i } \right) \right] \\ & = \frac { b - a } { N } \sum _ { i = 1 } ^ { N } \int _ { a } ^ { b } f ( x ) p ( x ) \mathrm { d } x \\ & = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \int _ { a } ^ { b } f ( x ) \mathrm { d } x \\ & = \int _ { a } ^ { b } f ( x ) \mathrm { d } x \end{aligned}
> $$

对均匀随机变量的限制可以用一个小的泛化来放宽。这是非常重要的一步，因为在蒙特卡罗中，仔细选择抽取样本的PDF是减少方差的一项重要技术。

如果随机变量 $X_i$ 服从某任意 PDF $p(x)$，则估计式
$$
F _ { N } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \frac { f \left( X _ { i } \right) } { p \left( X _ { i } \right) }
$$
的期望 $E \left[ F _ { N } \right]$ 是积分值，要求当 $f(x)\ne 0$ 时 $p(x)>0$。

> **推导** 
> $$
> \begin{aligned} E \left[ F _ { N } \right] & = E \left[ \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \frac { f \left( X _ { i } \right) } { p \left( X _ { i } \right) } \right] \\ & = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \int _ { a } ^ { b } \frac { f ( x ) } { p ( x ) } p ( x ) \mathrm { d } x \\ & = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \int _ { a } ^ { b } f ( x ) \mathrm { d } x \\ & = \int _ { a } ^ { b } f ( x ) \mathrm { d } x \end{aligned}
> $$

## 13.3 采样随机变量 Samping Random Variables

### 13.3.1 反演方法 the Inversion Method

反演方法使用一个或多个均匀随机变量，并将它们映射成服从期望分布的随机变量。

采样服从 PDF $p(x)$ 的 $X_i$ 的步骤

1. 计算 CDF $P(x)=\int_{-\infty}^x p(x')dx'$ 
2. 计算逆函数 $p^{-1}(x)$ 
3. 获取均匀分布的随机数 $\xi$ 
4. 计算 $X_i=P^{-1}(\xi)$ 

### 13.3.2 拒绝方法 the Rejection Method

对于有些函数 $f(x)$，难以积分它们来得到 PDFs，或者难以求逆 CDFs。拒绝方法只要求我们知道如何采样服从 PDF $p(x)$ 的随机变量，以及要求存在 $c$，使得 $f(x)<c\ p(x)$。算法如下

![1557133611082](assets/1557133611082.png)

拒绝方法并没有用在 `pbrt` 的蒙特卡洛算法里，一般更喜欢采样直接采样的方法。但这也是一个值得注意的重要方法。

## *13.4 大都市采样 Metropolis Sampling

## 13.5 分布变换 Transforming between Distributions

给定一个服从 PDF $p_x(x)$ 的随机变量 $X_i$，我们想知道随机变量 $Y_i=y(X_i)$ 的分布。函数 $y(x)$ 必须是单射的。即 y 的导数必须严格大于 0（或严格小于 0），意味着
$$
Pr\{Y\le y(x)\}=Pr\{X\le x\}
$$
即
$$
P_y(y)=P_y(y(x))=P_x(x)
$$
由 PDF 的定义可得微分关系为
$$
p_y(y)=\left|\frac{\mathrm{d}y}{\mathrm{d}x}\right|^{-1}p_x(x)
$$
已知 $X$ 服从 PDF $p_x(x)$，$Y$ 服从 PDF $p_y(y)$，要求累计分布函数相等，即 $P_y(y) = P_x(x)$，则
$$
y(x)=P_y^{-1}(P_x(x))
$$
如果 $X$ 是 $[0,1)$ 上均匀分布的随机变量，则 $P_x(x)=x$，这样我们就得到了 13.3.1 节给出的步骤。

### 13.5.1 多维变换 Transformation in Multiple Dimensions

假设我们有一个 n 维随机变量 $X$，其概率密度函数为 $p_x(x)$，令 $Y=T(X)$，且 $T$ 是一个双射，这样有
$$
p _ { y } ( y ) = p _ { y } ( T ( x ) ) = \frac { p _ { x } ( x ) } { \left| J _ { T } ( x ) \right| }
$$
其中 $T ( x ) = \left( T _ { 1 } ( x ) , \ldots , T _ { n } ( x ) \right)$，$|J_T|$ 是 $T$ 的雅克比矩阵的行列式的绝对值，雅克比矩阵如下
$$
\left( \begin{array} { c c c } { \partial T _ { 1 } / \partial x _ { 1 } } & { \cdots } & { \partial T _ { 1 } / \partial x _ { n } } \\ { \vdots } & { \ddots } & { \vdots } \\ { \partial T _ { n } / \partial x _ { 1 } } & { \cdots } & { \partial T _ { n } / \partial x _ { n } } \end{array} \right)
$$

### 13.5.2 极坐标 Polar Coordinates

极坐标变换为
$$
\begin{array} { l } { x = r \cos \theta } \\ { y = r \sin \theta } \end{array}
$$
假设我们从 $p(r,\theta)$ 中采样，想知道 $p(x,y)$，雅克比矩阵为
$$
J _ { T } = \left( \begin{array} { c c } { \frac { \partial x } { \partial r } } & { \frac { \partial x } { \partial \theta } } \\ { \frac { \partial y } { \partial r } } & { \frac { \partial y } { \partial \theta } } \end{array} \right) = \left( \begin{array} { c c } { \cos \theta } & { - r \sin \theta } \\ { \sin \theta } & { r \cos \theta } \end{array} \right)
$$
其行列式为 $r \left( \cos ^ { 2 } \theta + \sin ^ { 2 } \theta \right) = r$，因此 $p ( x , y ) = p ( r , \theta ) / r$。当然，这个与我们期望的相反了——通常我们从笛卡尔坐标采样然后变换它到极坐标中。这样我们有
$$
p ( r , \theta ) = r\ p ( x , y )
$$

### 13.5.3 球坐标 Spherical Coordinates

球坐标变换为
$$
\begin{aligned} x & = r \sin \theta \cos \phi \\ y & = r \sin \theta \sin \phi \\ z & = r \cos \theta \end{aligned}
$$
可求得 $\left| J _ { T } \right| = r ^ { 2 } \sin \theta$，相应的概率密度函数为
$$
p ( r , \theta , \phi ) = r ^ { 2 } \sin \theta p ( x , y , z )
$$

## 13.6 多维变换的二维采样 2D Sampling with Multidimensional Transformations

## 13.7 俄罗斯轮盘赌和分裂 Rissian Roulette and Spitting

## 13.8 仔细的样本布置 Careful Sample Placement

## 13.9 偏差 Bias

## 13.10 重要性采样 Importance Samping

